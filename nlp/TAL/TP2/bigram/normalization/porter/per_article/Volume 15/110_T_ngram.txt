('<s>', 'predict')	1	0.083333
('predict', 'the')	1	0.200000
('the', 'pandem')	1	0.090909
('pandem', 'sentiment')	1	1.000000
('sentiment', 'evalu')	2	0.333333
('evalu', 'and')	1	0.500000
('and', 'predict')	1	0.166667
('predict', 'analysi')	2	0.400000
('analysi', 'from')	1	0.200000
('from', 'large-scal')	1	0.200000
('large-scal', 'tweet')	1	1.000000
('tweet', 'on')	2	0.500000
('on', 'covid-19')	1	0.200000
('covid-19', 'by')	1	1.000000
('by', 'deep')	1	0.500000
('deep', 'convolut')	1	0.250000
('convolut', 'neural')	1	1.000000
('neural', 'network')	4	1.000000
('network', '</s>')	1	0.250000
('</s>', '<s>')	11	0.916667
('<s>', 'engag')	1	0.083333
('engag', 'deep')	1	1.000000
('deep', 'neural')	2	0.500000
('network', 'for')	1	0.250000
('for', 'textual')	1	0.125000
('textual', 'sentiment')	2	1.000000
('sentiment', 'analysi')	1	0.166667
('analysi', 'is')	1	0.200000
('is', 'an')	1	0.500000
('an', 'extens')	1	0.250000
('extens', 'practic')	1	1.000000
('practic', 'domain')	1	1.000000
('domain', 'of')	1	1.000000
('of', 'research')	1	0.250000
('research', '</s>')	1	0.500000
('<s>', 'textual')	1	0.083333
('sentiment', 'classif')	1	0.166667
('classif', 'har')	1	0.500000
('har', 'the')	1	1.000000
('the', 'full')	1	0.090909
('full', 'comput')	1	1.000000
('comput', 'potenti')	1	1.000000
('potenti', 'of')	1	1.000000
('of', 'deep')	1	0.250000
('deep', 'learn')	1	0.250000
('learn', 'model')	1	0.500000
('model', '</s>')	1	0.333333
('<s>', 'typic')	1	0.083333
('typic', 'these')	1	1.000000
('these', 'research')	1	1.000000
('research', 'work')	1	0.500000
('work', 'are')	1	1.000000
('are', 'carri')	1	1.000000
('carri', 'either')	1	1.000000
('either', 'with')	1	1.000000
('with', 'a')	1	0.500000
('a', 'popular')	1	0.100000
('popular', 'open-sourc')	2	1.000000
('open-sourc', 'data')	1	0.500000
('data', 'corpu')	1	0.166667
('corpu', 'or')	1	0.500000
('or', 'self-extract')	1	0.500000
('self-extract', 'short')	1	1.000000
('short', 'phrase')	1	1.000000
('phrase', 'text')	1	1.000000
('text', 'from')	1	0.333333
('from', 'twitter')	1	0.200000
('twitter', 'reddit')	1	1.000000
('reddit', 'or')	1	1.000000
('or', 'web-scrap')	1	0.500000
('web-scrap', 'text')	1	1.000000
('text', 'data')	1	0.333333
('data', 'from')	2	0.333333
('from', 'other')	1	0.200000
('other', 'resourc')	1	1.000000
('resourc', '</s>')	1	1.000000
('<s>', 'rare')	1	0.083333
('rare', 'do')	1	1.000000
('do', 'we')	1	1.000000
('we', 'see')	1	0.142857
('see', 'a')	1	1.000000
('a', 'larg')	2	0.200000
('larg', 'amount')	1	0.500000
('amount', 'of')	1	1.000000
('of', 'data')	1	0.250000
('data', 'on')	1	0.166667
('on', 'a')	1	0.200000
('a', 'current')	2	0.200000
('current', 'ongo')	2	1.000000
('ongo', 'event')	2	1.000000
('event', 'is')	1	0.500000
('is', 'be')	1	0.500000
('be', 'collect')	1	0.500000
('collect', 'and')	1	1.000000
('and', 'cultur')	1	0.166667
('cultur', 'further')	1	1.000000
('further', '</s>')	1	1.000000
('<s>', 'also')	1	0.083333
('also', 'an')	1	0.333333
('an', 'even')	1	0.250000
('even', 'more')	1	1.000000
('more', 'complex')	1	1.000000
('complex', 'task')	1	1.000000
('task', 'would')	1	0.500000
('would', 'be')	1	1.000000
('be', 'to')	1	0.500000
('to', 'model')	1	0.333333
('model', 'the')	1	0.333333
('the', 'data')	3	0.272727
('from', 'a')	1	0.200000
('event', 'not')	1	0.500000
('not', 'onli')	2	1.000000
('onli', 'for')	1	0.500000
('for', 'scale')	1	0.125000
('scale', 'the')	1	1.000000
('the', 'sentiment')	1	0.090909
('sentiment', 'accuraci')	2	0.333333
('accuraci', 'but')	1	0.250000
('but', 'also')	1	0.500000
('also', 'for')	1	0.333333
('for', 'make')	1	0.125000
('make', 'a')	1	1.000000
('a', 'predict')	1	0.100000
('analysi', 'for')	1	0.200000
('for', 'the')	1	0.125000
('the', 'same')	1	0.090909
('same', '</s>')	1	1.000000
('<s>', 'in')	1	0.083333
('in', 'thi')	1	0.500000
('thi', 'paper')	1	1.000000
('paper', 'we')	1	1.000000
('we', 'propos')	1	0.142857
('propos', 'a')	1	1.000000
('a', 'novel')	1	0.100000
('novel', 'approach')	1	1.000000
('approach', 'for')	1	1.000000
('for', 'achiev')	1	0.125000
('achiev', 'sentiment')	1	1.000000
('evalu', 'accuraci')	1	0.500000
('accuraci', 'by')	1	0.250000
('by', 'use')	1	0.500000
('use', 'a')	1	1.000000
('a', 'deep')	1	0.100000
('network', 'on')	1	0.250000
('on', 'live-stream')	1	0.200000
('live-stream', 'tweet')	1	1.000000
('on', 'coronaviru')	1	0.200000
('coronaviru', 'and')	1	0.333333
('and', 'futur')	1	0.166667
('futur', 'case')	1	0.500000
('case', 'growth')	2	0.666667
('growth', 'predict')	1	0.500000
('predict', '</s>')	1	0.200000
('<s>', 'we')	2	0.166667
('we', 'develop')	1	0.142857
('develop', 'a')	1	1.000000
('larg', 'tweet')	1	0.500000
('tweet', 'corpu')	1	0.250000
('corpu', 'exclus')	1	0.500000
('exclus', 'base')	1	1.000000
('base', 'on')	1	1.000000
('on', 'the')	1	0.200000
('the', 'coronaviru')	1	0.090909
('coronaviru', 'tweet')	1	0.333333
('tweet', '</s>')	1	0.250000
('we', 'split')	1	0.142857
('split', 'the')	1	1.000000
('data', 'into')	1	0.166667
('into', 'train')	1	1.000000
('train', 'and')	1	0.500000
('and', 'test')	1	0.166667
('test', 'set')	1	0.333333
('set', 'alongsid')	1	1.000000
('alongsid', 'we')	1	1.000000
('we', 'perform')	1	0.142857
('perform', 'polar')	1	0.500000
('polar', 'classif')	1	1.000000
('classif', 'and')	1	0.500000
('and', 'trend')	1	0.166667
('trend', 'analysi')	2	1.000000
('analysi', '</s>')	1	0.200000
('<s>', 'the')	1	0.083333
('the', 'refin')	1	0.090909
('refin', 'outcom')	1	1.000000
('outcom', 'from')	1	1.000000
('from', 'the')	1	0.200000
('the', 'trend')	1	0.090909
('analysi', 'help')	1	0.200000
('help', 'to')	1	1.000000
('to', 'train')	1	0.333333
('train', 'the')	1	0.500000
('data', 'to')	1	0.166667
('to', 'provid')	1	0.333333
('provid', 'an')	1	0.500000
('an', 'increment')	1	0.250000
('increment', 'learn')	1	1.000000
('learn', 'curvatur')	1	0.500000
('curvatur', 'for')	1	1.000000
('for', 'our')	1	0.125000
('our', 'neural')	1	0.500000
('network', 'and')	1	0.250000
('and', 'we')	1	0.166667
('we', 'obtain')	1	0.142857
('obtain', 'an')	1	1.000000
('an', 'accuraci')	1	0.250000
('accuraci', 'of')	1	0.250000
('of', '90.67%')	1	0.250000
('90.67%', '</s>')	1	1.000000
('<s>', 'final')	1	0.083333
('final', 'we')	1	1.000000
('we', 'provid')	1	0.142857
('provid', 'a')	1	0.500000
('a', 'statistical-bas')	1	0.100000
('statistical-bas', 'futur')	1	1.000000
('futur', 'predict')	1	0.500000
('predict', 'for')	1	0.200000
('for', 'coronaviru')	1	0.125000
('coronaviru', 'case')	1	0.333333
('growth', '</s>')	1	0.500000
('<s>', 'not')	1	0.083333
('onli', 'our')	1	0.500000
('our', 'model')	1	0.500000
('model', 'outperform')	1	0.333333
('outperform', 'sever')	1	1.000000
('sever', 'previou')	1	0.500000
('previou', 'state-of-art')	1	1.000000
('state-of-art', 'experi')	1	1.000000
('experi', 'in')	1	1.000000
('in', 'overal')	1	0.500000
('overal', 'sentiment')	1	1.000000
('accuraci', 'comparison')	1	0.250000
('comparison', 'for')	1	1.000000
('for', 'similar')	1	0.125000
('similar', 'task')	1	1.000000
('task', 'but')	1	0.500000
('but', 'it')	1	0.500000
('it', 'also')	1	1.000000
('also', 'maintain')	1	0.333333
('maintain', 'a')	1	1.000000
('a', 'throughout')	1	0.100000
('throughout', 'perform')	1	1.000000
('perform', 'stabil')	1	0.500000
('stabil', 'among')	1	1.000000
('among', 'all')	1	1.000000
('all', 'the')	1	1.000000
('the', 'test')	1	0.090909
('test', 'case')	1	0.333333
('case', 'when')	1	0.333333
('when', 'test')	1	1.000000
('test', 'with')	1	0.333333
('with', 'sever')	1	0.500000
('sever', 'popular')	1	0.500000
('open-sourc', 'text')	1	0.500000
('text', 'corpora')	1	0.333333
('corpora', '.')	1	1.000000
('.', '</s>')	1	1.000000
