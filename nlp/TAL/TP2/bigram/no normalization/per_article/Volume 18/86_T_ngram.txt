('<s>', 'A')	1	0.090909
('A', 'novel')	1	1.000000
('novel', 'fuzzy-entropy')	1	0.500000
('fuzzy-entropy', 'based')	1	1.000000
('based', 'online')	1	1.000000
('online', 'fuzzy')	1	0.500000
('fuzzy', 'C-Means')	2	0.666667
('C-Means', 'clustering')	1	0.200000
('clustering', 'algorithm')	1	0.200000
('algorithm', 'for')	2	0.333333
('for', 'massive')	1	0.200000
('massive', 'data')	1	0.500000
('data', '</s>')	2	0.666667
('</s>', '<s>')	10	0.909091
('<s>', 'Due')	1	0.090909
('Due', 'to')	1	1.000000
('to', 'the')	2	0.285714
('the', 'inadequacy')	1	0.058824
('inadequacy', 'of')	1	1.000000
('of', 'standard')	1	0.083333
('standard', 'clustering')	1	0.500000
('clustering', 'approaches')	1	0.200000
('approaches', 'for')	1	1.000000
('for', 'handling')	1	0.200000
('handling', 'extensive')	1	1.000000
('extensive', 'data')	1	1.000000
('data', 'considerable')	1	0.333333
('considerable', 'research')	1	1.000000
('research', 'has')	1	0.500000
('has', 'recently')	1	1.000000
('recently', 'focused')	1	1.000000
('focused', 'on')	1	1.000000
('on', 'clustering')	1	0.333333
('clustering', 'large')	2	0.400000
('large', 'and')	1	0.333333
('and', 'extremely')	1	0.250000
('extremely', 'large')	1	1.000000
('large', 'datasets')	1	0.333333
('datasets', '</s>')	1	0.250000
('<s>', 'Specifically')	1	0.090909
('Specifically', 'certain')	1	1.000000
('certain', 'variations')	1	1.000000
('variations', 'of')	1	1.000000
('of', 'the')	4	0.333333
('the', 'famous')	1	0.058824
('famous', 'fuzzy')	1	1.000000
('C-Means', 'algorithm')	1	0.200000
('algorithm', 'have')	1	0.166667
('have', 'been')	1	1.000000
('been', 'put')	1	1.000000
('put', 'forth')	1	1.000000
('forth', 'testing')	1	1.000000
('testing', 'techniques')	1	1.000000
('techniques', 'for')	1	0.500000
('for', 'segmenting')	1	0.200000
('segmenting', 'datasets')	1	1.000000
('datasets', 'and')	1	0.250000
('and', 'aggregating')	1	0.250000
('aggregating', 'the')	1	1.000000
('the', 'intermediate')	1	0.058824
('intermediate', 'clustered')	1	1.000000
('clustered', 'results')	1	1.000000
('results', '</s>')	1	0.500000
('<s>', 'Among')	1	0.090909
('Among', 'them')	1	1.000000
('them', 'the')	1	1.000000
('the', 'Fuzzy')	1	0.058824
('Fuzzy', 'C-Means')	3	1.000000
('C-Means', 'online')	1	0.200000
('online', 'technique')	1	0.500000
('technique', 'is')	1	1.000000
('is', 'one')	1	0.500000
('one', 'of')	1	1.000000
('the', 'most')	1	0.058824
('most', 'used')	1	1.000000
('used', 'for')	1	1.000000
('for', 'clustering')	1	0.200000
('large', 'amounts')	1	0.333333
('amounts', 'of')	1	1.000000
('of', 'data')	1	0.083333
('<s>', 'It')	1	0.090909
('It', 'splits')	1	1.000000
('splits', 'the')	1	1.000000
('the', 'dataset')	1	0.058824
('dataset', 'into')	1	1.000000
('into', 'equal-sized')	1	0.500000
('equal-sized', 'subsets')	1	1.000000
('subsets', 'or')	1	1.000000
('or', 'chunks')	1	1.000000
('chunks', 'and')	1	1.000000
('and', 'assigns')	1	0.250000
('assigns', 'a')	1	1.000000
('a', 'weight')	1	0.333333
('weight', 'to')	1	0.500000
('to', 'each')	1	0.142857
('each', 'chunk')	1	0.500000
('chunk', 'depending')	1	1.000000
('depending', 'on')	1	1.000000
('on', 'the')	1	0.333333
('the', 'membership')	1	0.058824
('membership', 'degrees')	1	1.000000
('degrees', 'per')	1	1.000000
('per', 'cluster')	1	1.000000
('cluster', '</s>')	2	0.666667
('<s>', 'This')	1	0.090909
('This', 'study')	1	1.000000
('study', 'introduces')	1	1.000000
('introduces', 'a')	1	1.000000
('a', 'novel')	1	0.333333
('novel', 'variation')	1	0.500000
('variation', 'of')	1	1.000000
('the', 'Online')	1	0.058824
('Online', 'Fuzzy')	1	1.000000
('C-Means', 'OFCM')	1	0.200000
('OFCM', 'algorithm')	1	0.500000
('algorithm', 'designed')	1	0.166667
('designed', 'to')	1	1.000000
('to', 'boost')	1	0.142857
('boost', 'its')	1	1.000000
('its', 'performance')	1	1.000000
('performance', '</s>')	1	0.500000
('<s>', 'Our')	1	0.090909
('Our', 'proposed')	1	1.000000
('proposed', 'method')	1	0.333333
('method', 'integrates')	1	1.000000
('integrates', 'a')	1	1.000000
('a', 'cluster')	1	0.333333
('cluster', 'compactness')	1	0.333333
('compactness', 'measure')	1	1.000000
('measure', 'into')	1	1.000000
('into', 'the')	1	0.500000
('the', 'weight')	1	0.058824
('weight', 'attribution')	1	0.500000
('attribution', 'process')	1	1.000000
('process', 'quantified')	1	1.000000
('quantified', 'by')	1	1.000000
('by', 'the')	1	0.500000
('the', 'fuzzy')	1	0.058824
('fuzzy', 'entropy')	1	0.333333
('entropy', 'of')	1	1.000000
('of', 'each')	1	0.083333
('each', 'cluster')	1	0.500000
('<s>', 'Comparative')	1	0.090909
('Comparative', 'experiments')	1	1.000000
('experiments', 'conducted')	1	1.000000
('conducted', 'across')	1	1.000000
('across', 'diverse')	1	1.000000
('diverse', 'classification')	1	1.000000
('classification', 'datasets')	1	1.000000
('datasets', 'of')	1	0.250000
('of', 'varying')	1	0.083333
('varying', 'scales')	1	1.000000
('scales', 'demonstrate')	1	1.000000
('demonstrate', 'that')	1	1.000000
('that', 'the')	1	0.500000
('the', 'proposed')	2	0.117647
('proposed', 'algorithm')	2	0.666667
('algorithm', 'significantly')	1	0.166667
('significantly', 'improves')	1	1.000000
('improves', 'the')	1	1.000000
('the', 'accuracy')	1	0.058824
('accuracy', 'of')	1	1.000000
('of', 'clustering')	1	0.083333
('clustering', 'results')	1	0.200000
('results', 'when')	1	0.500000
('when', 'compared')	1	1.000000
('compared', 'to')	1	1.000000
('the', 'standard')	1	0.058824
('standard', 'OFCM')	1	0.500000
('OFCM', '</s>')	1	0.500000
('<s>', 'Crucially')	1	0.090909
('Crucially', 'this')	1	1.000000
('this', 'enhancement')	1	1.000000
('enhancement', 'is')	1	1.000000
('is', 'achieved')	1	0.500000
('achieved', 'without')	1	1.000000
('without', 'increasing')	1	1.000000
('increasing', 'the')	1	1.000000
('the', 'computational')	1	0.058824
('computational', 'complexity')	1	1.000000
('complexity', 'of')	1	1.000000
('the', 'algorithm')	1	0.058824
('algorithm', '</s>')	1	0.166667
('<s>', 'Furthermore')	1	0.090909
('Furthermore', 'our')	1	1.000000
('our', 'approach')	1	1.000000
('approach', 'yields')	1	1.000000
('yields', 'performance')	1	1.000000
('performance', 'comparable')	1	0.500000
('comparable', 'to')	1	1.000000
('to', 'that')	1	0.142857
('that', 'of')	1	0.500000
('of', 'heuristic')	1	0.083333
('heuristic', 'Fuzzy')	1	1.000000
('C-Means', 'algorithms')	1	0.200000
('algorithms', 'while')	1	1.000000
('while', 'offering')	1	1.000000
('offering', 'the')	1	1.000000
('the', 'distinct')	1	0.058824
('distinct', 'advantage')	1	1.000000
('advantage', 'of')	1	1.000000
('of', 'shorter')	1	0.083333
('shorter', 'execution')	1	1.000000
('execution', 'times')	1	1.000000
('times', '</s>')	1	1.000000
('<s>', 'Future')	1	0.090909
('Future', 'research')	1	1.000000
('research', 'will')	1	0.500000
('will', 'focus')	1	1.000000
('focus', 'on')	1	1.000000
('on', 'exploring')	1	0.333333
('exploring', 'feature')	1	1.000000
('feature', 'selection')	1	1.000000
('selection', 'and')	1	1.000000
('and', 'reduction')	1	0.250000
('reduction', 'techniques')	1	1.000000
('techniques', 'to')	1	0.500000
('to', 'adapt')	1	0.142857
('adapt', 'the')	1	1.000000
('for', 'effective')	1	0.200000
('effective', 'application')	1	1.000000
('application', 'to')	1	1.000000
('to', 'massive')	1	0.142857
('massive', 'datasets')	1	0.500000
('datasets', 'characterized')	1	0.250000
('characterized', 'by')	1	1.000000
('by', 'an')	1	0.500000
('an', 'exceptionally')	1	1.000000
('exceptionally', 'high')	1	1.000000
('high', 'number')	1	1.000000
('number', 'of')	1	1.000000
('of', 'features')	1	0.083333
('features', '.')	1	1.000000
('.', '</s>')	1	1.000000
