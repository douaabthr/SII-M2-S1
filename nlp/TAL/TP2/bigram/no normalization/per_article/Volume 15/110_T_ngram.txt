('<s>', 'Predicting')	1	0.083333
('Predicting', 'the')	1	1.000000
('the', 'pandemic')	1	0.100000
('pandemic', 'sentiment')	1	1.000000
('sentiment', 'evaluation')	2	0.333333
('evaluation', 'and')	1	0.500000
('and', 'predictive')	1	0.166667
('predictive', 'analysis')	2	1.000000
('analysis', 'from')	1	0.200000
('from', 'large-scale')	1	0.200000
('large-scale', 'tweets')	1	1.000000
('tweets', 'on')	2	0.666667
('on', 'Covid-19')	1	0.200000
('Covid-19', 'by')	1	1.000000
('by', 'deep')	1	0.500000
('deep', 'convolutional')	1	0.250000
('convolutional', 'neural')	1	1.000000
('neural', 'network')	3	0.750000
('network', '</s>')	1	0.333333
('</s>', '<s>')	11	0.916667
('<s>', 'Engaging')	1	0.083333
('Engaging', 'deep')	1	1.000000
('deep', 'neural')	2	0.500000
('neural', 'networks')	1	0.250000
('networks', 'for')	1	1.000000
('for', 'textual')	1	0.125000
('textual', 'sentiment')	1	1.000000
('sentiment', 'analysis')	1	0.166667
('analysis', 'is')	1	0.200000
('is', 'an')	1	0.500000
('an', 'extensively')	1	0.250000
('extensively', 'practiced')	1	1.000000
('practiced', 'domain')	1	1.000000
('domain', 'of')	1	1.000000
('of', 'research')	1	0.250000
('research', '</s>')	1	0.500000
('<s>', 'Textual')	1	0.083333
('Textual', 'sentiment')	1	1.000000
('sentiment', 'classification')	1	0.166667
('classification', 'harnesses')	1	0.500000
('harnesses', 'the')	1	1.000000
('the', 'full')	1	0.100000
('full', 'computational')	1	1.000000
('computational', 'potential')	1	1.000000
('potential', 'of')	1	1.000000
('of', 'deep')	1	0.250000
('deep', 'learning')	1	0.250000
('learning', 'models')	1	0.500000
('models', '</s>')	1	1.000000
('<s>', 'Typically')	1	0.083333
('Typically', 'these')	1	1.000000
('these', 'research')	1	1.000000
('research', 'works')	1	0.500000
('works', 'are')	1	1.000000
('are', 'carried')	1	1.000000
('carried', 'either')	1	1.000000
('either', 'with')	1	1.000000
('with', 'a')	1	0.500000
('a', 'popular')	1	0.100000
('popular', 'open-source')	2	1.000000
('open-source', 'data')	1	0.500000
('data', 'corpus')	1	0.166667
('corpus', 'or')	1	0.500000
('or', 'self-extracted')	1	0.500000
('self-extracted', 'short')	1	1.000000
('short', 'phrase')	1	1.000000
('phrase', 'texts')	1	1.000000
('texts', 'from')	1	1.000000
('from', 'Twitter')	1	0.200000
('Twitter', 'Reddit')	1	1.000000
('Reddit', 'or')	1	1.000000
('or', 'web-scrapped')	1	0.500000
('web-scrapped', 'text')	1	1.000000
('text', 'data')	1	0.500000
('data', 'from')	2	0.333333
('from', 'other')	1	0.200000
('other', 'resources')	1	1.000000
('resources', '</s>')	1	1.000000
('<s>', 'Rarely')	1	0.083333
('Rarely', 'do')	1	1.000000
('do', 'we')	1	1.000000
('we', 'see')	1	0.200000
('see', 'a')	1	1.000000
('a', 'large')	2	0.200000
('large', 'amount')	1	0.500000
('amount', 'of')	1	1.000000
('of', 'data')	1	0.250000
('data', 'on')	1	0.166667
('on', 'a')	1	0.200000
('a', 'current')	1	0.100000
('current', 'ongoing')	1	1.000000
('ongoing', 'event')	2	1.000000
('event', 'is')	1	0.500000
('is', 'being')	1	0.500000
('being', 'collected')	1	1.000000
('collected', 'and')	1	1.000000
('and', 'cultured')	1	0.166667
('cultured', 'further')	1	1.000000
('further', '</s>')	1	1.000000
('<s>', 'Also')	1	0.083333
('Also', 'an')	1	1.000000
('an', 'even')	1	0.250000
('even', 'more')	1	1.000000
('more', 'complex')	1	1.000000
('complex', 'task')	1	1.000000
('task', 'would')	1	1.000000
('would', 'be')	1	1.000000
('be', 'to')	1	1.000000
('to', 'model')	1	0.333333
('model', 'the')	1	0.500000
('the', 'data')	3	0.300000
('from', 'a')	1	0.200000
('a', 'currently')	1	0.100000
('currently', 'ongoing')	1	1.000000
('event', 'not')	1	0.500000
('not', 'only')	1	1.000000
('only', 'for')	1	0.500000
('for', 'scaling')	1	0.125000
('scaling', 'the')	1	1.000000
('the', 'sentiment')	1	0.100000
('sentiment', 'accuracy')	2	0.333333
('accuracy', 'but')	1	0.250000
('but', 'also')	1	0.500000
('also', 'for')	1	0.500000
('for', 'making')	1	0.125000
('making', 'a')	1	1.000000
('a', 'predictive')	1	0.100000
('analysis', 'for')	1	0.200000
('for', 'the')	1	0.125000
('the', 'same')	1	0.100000
('same', '</s>')	1	1.000000
('<s>', 'In')	1	0.083333
('In', 'this')	1	1.000000
('this', 'paper')	1	1.000000
('paper', 'we')	1	1.000000
('we', 'propose')	1	0.200000
('propose', 'a')	1	1.000000
('a', 'novel')	1	0.100000
('novel', 'approach')	1	1.000000
('approach', 'for')	1	1.000000
('for', 'achieving')	1	0.125000
('achieving', 'sentiment')	1	1.000000
('evaluation', 'accuracy')	1	0.500000
('accuracy', 'by')	1	0.250000
('by', 'using')	1	0.500000
('using', 'a')	1	1.000000
('a', 'deep')	1	0.100000
('network', 'on')	1	0.333333
('on', 'live-streamed')	1	0.200000
('live-streamed', 'tweets')	1	1.000000
('on', 'Coronavirus')	1	0.200000
('Coronavirus', 'and')	1	0.333333
('and', 'future')	1	0.166667
('future', 'case')	1	0.500000
('case', 'growth')	1	1.000000
('growth', 'prediction')	1	0.500000
('prediction', '</s>')	1	0.500000
('<s>', 'We')	2	0.166667
('We', 'develop')	1	0.500000
('develop', 'a')	1	1.000000
('large', 'tweet')	1	0.500000
('tweet', 'corpus')	1	1.000000
('corpus', 'exclusively')	1	0.500000
('exclusively', 'based')	1	1.000000
('based', 'on')	1	1.000000
('on', 'the')	1	0.200000
('the', 'Coronavirus')	1	0.100000
('Coronavirus', 'tweets')	1	0.333333
('tweets', '</s>')	1	0.333333
('We', 'split')	1	0.500000
('split', 'the')	1	1.000000
('data', 'into')	1	0.166667
('into', 'train')	1	1.000000
('train', 'and')	1	0.500000
('and', 'test')	1	0.166667
('test', 'sets')	1	0.500000
('sets', 'alongside')	1	1.000000
('alongside', 'we')	1	1.000000
('we', 'perform')	1	0.200000
('perform', 'polarity')	1	1.000000
('polarity', 'classification')	1	1.000000
('classification', 'and')	1	0.500000
('and', 'trend')	1	0.166667
('trend', 'analysis')	2	1.000000
('analysis', '</s>')	1	0.200000
('<s>', 'The')	1	0.083333
('The', 'refined')	1	1.000000
('refined', 'outcome')	1	1.000000
('outcome', 'from')	1	1.000000
('from', 'the')	1	0.200000
('the', 'trend')	1	0.100000
('analysis', 'helps')	1	0.200000
('helps', 'to')	1	1.000000
('to', 'train')	1	0.333333
('train', 'the')	1	0.500000
('data', 'to')	1	0.166667
('to', 'provide')	1	0.333333
('provide', 'an')	1	0.500000
('an', 'incremental')	1	0.250000
('incremental', 'learning')	1	1.000000
('learning', 'curvature')	1	0.500000
('curvature', 'for')	1	1.000000
('for', 'our')	1	0.125000
('our', 'neural')	1	0.500000
('network', 'and')	1	0.333333
('and', 'we')	1	0.166667
('we', 'obtain')	1	0.200000
('obtain', 'an')	1	1.000000
('an', 'accuracy')	1	0.250000
('accuracy', 'of')	1	0.250000
('of', '90.67%')	1	0.250000
('90.67%', '</s>')	1	1.000000
('<s>', 'Finally')	1	0.083333
('Finally', 'we')	1	1.000000
('we', 'provide')	1	0.200000
('provide', 'a')	1	0.500000
('a', 'statistical-based')	1	0.100000
('statistical-based', 'future')	1	1.000000
('future', 'prediction')	1	0.500000
('prediction', 'for')	1	0.500000
('for', 'Coronavirus')	1	0.125000
('Coronavirus', 'cases')	1	0.333333
('cases', 'growth')	1	0.500000
('growth', '</s>')	1	0.500000
('<s>', 'Not')	1	0.083333
('Not', 'only')	1	1.000000
('only', 'our')	1	0.500000
('our', 'model')	1	0.500000
('model', 'outperforms')	1	0.500000
('outperforms', 'several')	1	1.000000
('several', 'previous')	1	0.500000
('previous', 'state-of-art')	1	1.000000
('state-of-art', 'experiments')	1	1.000000
('experiments', 'in')	1	1.000000
('in', 'overall')	1	1.000000
('overall', 'sentiment')	1	1.000000
('accuracy', 'comparison')	1	0.250000
('comparison', 'for')	1	1.000000
('for', 'similar')	1	0.125000
('similar', 'tasks')	1	1.000000
('tasks', 'but')	1	1.000000
('but', 'it')	1	0.500000
('it', 'also')	1	1.000000
('also', 'maintains')	1	0.500000
('maintains', 'a')	1	1.000000
('a', 'throughout')	1	0.100000
('throughout', 'performance')	1	1.000000
('performance', 'stability')	1	1.000000
('stability', 'among')	1	1.000000
('among', 'all')	1	1.000000
('all', 'the')	1	1.000000
('the', 'test')	1	0.100000
('test', 'cases')	1	0.500000
('cases', 'when')	1	0.500000
('when', 'tested')	1	1.000000
('tested', 'with')	1	1.000000
('with', 'several')	1	0.500000
('several', 'popular')	1	0.500000
('open-source', 'text')	1	0.500000
('text', 'corpora')	1	0.500000
('corpora', '.')	1	1.000000
('.', '</s>')	1	1.000000
