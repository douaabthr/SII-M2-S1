Analyzing metaheuristic algorithms performance and the causes of the zero-bias problem: a different perspective in benchmarks

This paper presents an alternative approach to evaluate the explorative capabilities of metaheuristic algorithms by analyzing their performance on benchmark functions. Unlike conventional methods that focus on the precision of final solutions, the proposed method evaluates how often algorithms locate the region containing the global optimum. By defining a threshold fitness value that distinguishes between global and local optima, the study provides a direct measure of exploration success. Eleven metaheuristic algorithms were compared on multimodal and shifted benchmark functions, uncovering significant differences in their exploration efficiency. Notably, a pervasive flaw—the zero-bias problem—was identified in several algorithms, which artificially boosts performance on certain benchmarks while hindering real-world applicability. This paper proposes fixes for these issues and highlights standout performers with robust exploration capabilities. The findings offer researchers a clearer understanding of exploration mechanisms and provide practical insights for designing more effective optimization algorithms.
