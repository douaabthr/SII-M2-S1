Enhanced snow ablation optimizer using advanced quadratic and newton interpolation with taylor neighbourhood search and second-order differential perturbation strategies for high-dimensional feature selection
Feature selection is essential in data mining and machine learning significantly enhancing classification accuracy and reducing computational overhead
As datasets increase in size identifying optimal feature subsets becomes more complex necessitating the use of metaheuristic optimization techniques
Unlike traditional methods these techniques effectively navigate extensive search spaces
The Snow Ablation Optimizer SAO a novel and widely adopted nature-inspired metaheuristic is recognized for its simplicity and optimization efficacy
Nevertheless SAO like other nature-inspired algorithms encounters limitations such as accuracy constraints restricted population diversity and premature convergence particularly in complex high-dimensional datasets
To address these limitations this study proposes an improved variant of the recent optimization algorithm known as the Enhanced Snow Ablation Optimizer ESAO along with a wrapper-based feature selection FS method that utilizes the k-nearest neighbour KNN classifier
In the initial phase the proposed algorithm employs a position update mechanism using advanced quadratic and Newton interpolation operators to navigate complex search spaces and accelerate convergence
Taylor neighbourhood strategies broaden the search scope enhancing exploration and solution discovery while minimizing the risk of local optima
Second-order differential perturbation strategies further improve exploration and exploitation by capturing objective function nonlinearities ensuring more accurate and efficient optimization and promoting diverse solutions
The SAO algorithm originally formulated for continuous search spaces currently employs eight transfer functions S-shaped V-shaped Z-shaped and three U-shaped customized to address optimization challenges within binary spaces
Its efficacy was validated using 21 high-dimensional datasets and compared against eight competitor algorithms
Subsequently the Technique for Order of Preference by Similarity to Ideal Solution TOPSIS method assesses optimal transfer functions based on classification accuracy highlighting significant progress with accuracies ranging from 88 to 100% .