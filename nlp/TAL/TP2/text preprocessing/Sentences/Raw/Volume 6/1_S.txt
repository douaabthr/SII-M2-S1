Scalable multiagent learning through indirect encoding of policy geometry
Multiagent systems present many challenging real-world problems to artificial intelligence
Because it is difficult to engineer the behaviors of multiple cooperating agents by hand multiagent learninghas become a popular approach to their design
While there are a variety of traditional approaches to multiagent learning many suffer from increased computational costs for large teams and theproblem of reinvention that is the inability to recognize that certain skills are shared by some or all team member
This paper presents an alternative approach to multiagent learning calledmultiagent HyperNEATthat represents the team as apatternof policies rather than as a set of individual agents
The main idea is that an agent s location within a canonical team layout which can be physical such as positions on a sports team or conceptual such as an agent s relative speed tends to dictate its role within that team
This paper introduces the termpolicy geometryto describe this relationship between role and position on the team
Interestingly such patterns effectively represent up to an infinite number of multiagent policies that can be sampled from the policy geometry as needed to allow training very large teams or in some cases scaling up the size of a team without additional learning
In this paper multiagent HyperNEAT is compared to a traditional learning method multiagent Sarsa Î» in a predator prey domain where it demonstrates its ability to train large teams .