Tuning and evolution of support vector kernels
Kernel-based methods like Support Vector Machines SVM have been established as powerful techniques in machine learning
The idea of SVM is to perform a mapping from the input space to a higher-dimensional feature space using a kernel function so that a linear learning algorithm can be employed
However the burden of choosing the appropriate kernel function is usually left to the user
It can easily be shown that the accuracy of the learned model highly depends on the chosen kernel function and its parameters especially for complex tasks
In order to obtain a good classification or regression model an appropriate kernel function in combination with optimized pre and post-processed data must be used
To circumvent these obstacles we present two solutions for optimizing kernel functions a automated hyperparameter tuning of kernel functions combined with an optimization of pre and post-processing options by Sequential Parameter Optimization SPO and b evolving new kernel functions by Genetic Programming GP
We review modern techniques for both approaches comparing their different strengths and weaknesses
We apply tuning to SVM kernels for both regression and classification
Automatic hyperparameter tuning of standard kernels and pre and post-processing options always yielded to systems with excellent prediction accuracy on the considered problems
Especially SPO-tuned kernels lead to much better results than all other tested tuning approaches
Regarding GP-based kernel evolution our method rediscovered multiple standard kernels but no significant improvements over standard kernels were obtained .