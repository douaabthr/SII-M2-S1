Evolution of visual controllers for obstacle avoidance in mobile robotics
The purpose of this work is to automatically design vision algorithms for a mobile robot adapted to its current visual context
In this paper we address the particular task of obstacle avoidance using monocular vision
Starting from a set of primitives composed of the different techniques found in the literature we propose a generic structure to represent the algorithms using standard resolution video sequences as an input and velocity commands to control a wheel robot as an output
Grammar rules are then used to construct correct instances of algorithms that are then evaluated using different protocols evaluation of trajectories performed in a goal reaching task or imitation of a hand-guided trajectory
A genetic program is applied to evolve populations of algorithms in order to optimize the performances of the controllers
The first results obtained in a simulated environment show that the evolution produces algorithms that can be easily interpreted and which are clearly adapted to the visual context
However the resulting trajectories are often erratic and the generalization capacities are poor
To improve the results we propose to use a two-phase evolution combining imitation and goal reaching evaluations and to add some constraints in the grammar rules to enforce a more generic behavior
The results obtained in simulation show that the evolved algorithms are more efficient and more generic
Finally we apply the imitation based evolution on real sequences and test the evolved algorithms on a real robot
Though simplified by dropping the goal reaching constraint the resulting algorithms behave well in a corridor centering task and show certain generalization capacities .