<s>
modSwish
a
new
activation
function
for
neural
network
</s>
<s>
The
activation
functions
are
extremely
important
to
neural
networks
since
they
are
responsible
for
learning
the
abstract
characteristics
of
the
data
through
nonlinear
modification
</s>
<s>
The
paper
presents
a
new
activation
function
which
is
referred
to
as
modSwish
</s>
<s>
It
is
continuously
differentiable
unbounded
above
bounded
below
and
non-monotonic
</s>
<s>
Our
results
demonstrate
that
modSwish
outperforms
ReLU
on
a
number
of
challenging
datasets
and
neural
network
models
</s>
<s>
In
the
beginning
of
the
experiment
Neural
Networks
are
trained
and
classified
using
benchmark
data
like
Diagnostic
Wisconsin
Breast
Cancer
and
Iris
and
modSwish
achieved
93.57%
and
95.56%
accuracy
respectively
</s>
<s>
Secondly
experiments
were
conducted
on
five
distinct
neural
network
depths
that
ranged
from
five
to
eight
layers
over
MNIST
datasets
</s>
<s>
The
modSwish
activation
function
obtained
95.57%
95.29%
94.93%
94.69%
and
95.03%
accuracy
on
8
layers
7
layers
6
layers
5
thinner
layers
and
wider
5
layers
neural
networks
respectively
</s>
<s>
Finally
experiments
were
conducted
on
two
distinct
Convolution
Neural
Network
with
two
convolution
layer
and
four
convolution
layers
over
CIFAR-10
datasets
</s>
<s>
The
modSwish
activation
function
obtained
60.04%
and
69.22%
accuracy
on
two
convolution
layer
and
four
convolution
layer
model
respectively
</s>
<s>
Statistical
feature
measurements
demonstrate
that
modSwish
has
the
best
mean
accuracy
lowest
Root
Mean
squared
Error
lowest
standard
deviation
lowest
variance
and
lowest
Mean
squared
Error
</s>
<s>
The
study
indicated
that
modSwish
has
faster
convergence
compared
to
ReLU
making
it
a
valuable
factor
in
deep
learning
</s>
<s>
The
results
of
the
experiments
suggest
that
modSwish
can
be
a
promising
substitute
for
ReLU
leading
to
better
performance
in
neural
network
models
.
</s>