<s>
Gaussian
mixture
models
for
training
Bayesian
convolutional
neural
networks
</s>
<s>
Bayes
by
Backprop
is
a
variational
inference
method
based
on
the
reparametrization
trick
to
assure
backpropagation
in
Bayesian
neural
networks
</s>
<s>
Generally
the
approximate
distributions
used
in
Bayes
by
backprop
method
are
made
unimodal
to
facilitate
the
use
of
the
reparametrization
trick
</s>
<s>
But
frequently
the
modelling
of
some
tasks
requires
more
sophisticated
distributions
</s>
<s>
This
paper
describes
the
Bayes
by
Backprop
algorithm
with
a
multi-model
distribution
for
training
Bayesian
convolutional
neural
networks
</s>
<s>
Specifically
we
illustrate
how
to
reparameterize
the
CNN
parameters
for
a
Gaussian
mixture
model
</s>
<s>
We
then
show
that
the
results
compare
favourably
to
existing
variational
algorithms
on
various
classification
datasets
</s>
<s>
Finally
we
illustrate
how
to
use
this
distribution
to
estimate
epistemic
and
aleatoric
uncertainty
.
</s>