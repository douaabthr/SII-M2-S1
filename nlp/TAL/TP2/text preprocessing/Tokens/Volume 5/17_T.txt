<s>
Efficient
recurrent
local
search
strategies
for
semi
and
unsupervised
regularized
least-squares
classification
</s>
<s>
Binary
classification
tasks
are
among
the
most
important
ones
in
the
field
of
machine
learning
</s>
<s>
One
prominent
approach
to
address
such
tasks
are
support
vector
machines
which
aim
at
finding
a
hyperplane
separating
two
classes
well
such
that
the
induced
distance
between
the
hyperplane
and
the
patterns
is
maximized
</s>
<s>
In
general
sufficient
labeled
data
is
needed
for
such
classification
settings
to
obtain
reasonable
models
</s>
<s>
However
labeled
data
is
often
rare
in
real-world
learning
scenarios
while
unlabeled
data
can
be
obtained
easily
</s>
<s>
For
this
reason
the
concept
of
support
vector
machines
has
also
been
extended
to
semi
and
unsupervised
settings
in
the
unsupervised
case
one
aims
at
finding
a
partition
of
the
data
into
two
classes
such
that
a
subsequent
application
of
a
support
vector
machine
leads
to
the
best
overall
result
</s>
<s>
Similarly
given
both
a
labeled
and
an
unlabeled
part
semi-supervised
support
vector
machines
favor
decision
hyperplanes
that
lie
in
a
low
density
area
induced
by
the
unlabeled
training
patterns
while
still
considering
the
labeled
part
of
the
data
</s>
<s>
The
associated
optimization
problems
for
both
the
semi
and
unsupervised
case
however
are
of
combinatorial
nature
and
hence
difficult
to
solve
</s>
<s>
In
this
work
we
present
efficient
implementations
of
simple
local
search
strategies
for
variants
of
the
both
cases
that
are
based
on
matrix
update
schemes
for
the
intermediate
candidate
solutions
</s>
<s>
We
evaluate
the
performances
of
the
resulting
approaches
on
a
variety
of
artificial
and
real-world
data
sets
</s>
<s>
The
results
indicate
that
our
approaches
can
successfully
incorporate
unlabeled
data
</s>
<s>
The
unsupervised
case
was
originally
proposed
by
Gieseke
F
Pahikkala
et
al
</s>
<s>
2009
</s>
<s>
The
derivations
presented
in
this
work
are
new
and
comprehend
the
old
ones
for
the
unsupervised
setting
as
a
special
case
.
</s>