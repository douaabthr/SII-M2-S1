<s>
Kernel
representations
for
evolving
continuous
functions
</s>
<s>
To
parameterize
continuous
functions
for
evolutionary
learning
we
use
kernel
expansions
in
nested
sequences
of
function
spaces
of
growing
complexity
</s>
<s>
This
approach
is
particularly
powerful
when
dealing
with
non-convex
constraints
and
discontinuous
objective
functions
</s>
<s>
Kernel
methods
offer
a
number
of
beneficial
properties
for
parameterizing
continuous
functions
such
as
smoothness
and
locality
which
make
them
attractive
as
a
basis
for
mutation
operators
</s>
<s>
Beyond
such
practical
considerations
kernel
methods
make
heavy
use
of
inner
products
in
function
space
and
offer
a
well
established
regularization
framework
</s>
<s>
We
show
how
evolutionary
computation
can
profit
from
these
properties
</s>
<s>
Searching
function
spaces
of
iteratively
increasing
complexity
allows
the
solution
to
evolve
from
a
simple
first
guess
to
a
complex
and
highly
refined
function
</s>
<s>
At
transition
points
where
the
evolution
strategy
is
confronted
with
the
next
level
of
functional
complexity
the
kernel
framework
can
be
used
to
project
the
search
distribution
into
the
extended
search
space
</s>
<s>
The
feasibility
of
the
method
is
demonstrated
on
challenging
trajectory
planning
problems
where
redundant
robots
have
to
avoid
obstacles
.
</s>