<s>
Tuning
and
evolution
of
support
vector
kernels
</s>
<s>
Kernel-based
methods
like
Support
Vector
Machines
SVM
have
been
established
as
powerful
techniques
in
machine
learning
</s>
<s>
The
idea
of
SVM
is
to
perform
a
mapping
from
the
input
space
to
a
higher-dimensional
feature
space
using
a
kernel
function
so
that
a
linear
learning
algorithm
can
be
employed
</s>
<s>
However
the
burden
of
choosing
the
appropriate
kernel
function
is
usually
left
to
the
user
</s>
<s>
It
can
easily
be
shown
that
the
accuracy
of
the
learned
model
highly
depends
on
the
chosen
kernel
function
and
its
parameters
especially
for
complex
tasks
</s>
<s>
In
order
to
obtain
a
good
classification
or
regression
model
an
appropriate
kernel
function
in
combination
with
optimized
pre
and
post-processed
data
must
be
used
</s>
<s>
To
circumvent
these
obstacles
we
present
two
solutions
for
optimizing
kernel
functions
a
automated
hyperparameter
tuning
of
kernel
functions
combined
with
an
optimization
of
pre
and
post-processing
options
by
Sequential
Parameter
Optimization
SPO
and
b
evolving
new
kernel
functions
by
Genetic
Programming
GP
</s>
<s>
We
review
modern
techniques
for
both
approaches
comparing
their
different
strengths
and
weaknesses
</s>
<s>
We
apply
tuning
to
SVM
kernels
for
both
regression
and
classification
</s>
<s>
Automatic
hyperparameter
tuning
of
standard
kernels
and
pre
and
post-processing
options
always
yielded
to
systems
with
excellent
prediction
accuracy
on
the
considered
problems
</s>
<s>
Especially
SPO-tuned
kernels
lead
to
much
better
results
than
all
other
tested
tuning
approaches
</s>
<s>
Regarding
GP-based
kernel
evolution
our
method
rediscovered
multiple
standard
kernels
but
no
significant
improvements
over
standard
kernels
were
obtained
.
</s>