<s>
Towards
robot
vision
using
deep
neural
networks
in
evolutionary
robotics
</s>
<s>
In
evolutionary
robotics
robot
controllers
are
often
evolved
in
simulation
as
using
the
physical
robot
for
fitness
evaluation
can
take
a
prohibitively
long
time
</s>
<s>
Simulators
provide
a
quick
way
to
evaluate
controller
fitness
</s>
<s>
A
simulator
is
tasked
with
providing
appropriate
sensor
information
to
the
controller
</s>
<s>
If
the
robot
has
an
on-board
camera
an
entire
virtual
visual
environment
is
needed
to
simulate
the
camera
s
signal
</s>
<s>
In
the
past
these
visual
environments
have
been
constructed
by
hand
requiring
the
use
of
hand-crafted
models
textures
and
lighting
which
is
a
tedious
and
time-consuming
process
</s>
<s>
This
paper
proposes
a
deep
neural
network-based
architecture
for
simulating
visual
environments
</s>
<s>
The
neural
networks
are
trained
exclusively
from
images
captured
from
the
robot
creating
a
3
dimensional
visual
environment
without
using
hand-crafted
models
textures
or
lighting
</s>
<s>
It
does
not
rely
on
any
external
domain
specific
datasets
as
all
training
data
is
captured
in
the
physical
environment
</s>
<s>
Robot
controllers
were
evolved
in
simulation
to
discern
between
objects
with
different
colours
and
shapes
and
they
successfully
completed
the
same
task
in
the
real
world
.
</s>