<s>
Mutual
information
for
feature
selection
estimation
or
counting
</s>
<s>
</s>
<s>
In
classification
feature
selection
is
an
important
pre-processing
step
to
simplify
the
dataset
and
improve
the
data
representation
quality
which
makes
classifiers
become
better
easier
to
train
and
understand
</s>
<s>
Because
of
an
ability
to
analyse
non-linear
interactions
between
features
mutual
information
has
been
widely
applied
to
feature
selection
</s>
<s>
Along
with
counting
approaches
a
traditional
way
to
calculate
mutual
information
many
mutual
information
estimations
have
been
proposed
to
allow
mutual
information
to
work
directly
on
continuous
datasets
</s>
<s>
This
work
focuses
on
comparing
the
effect
of
counting
approach
and
kernel
density
estimation
KDE
approach
in
feature
selection
using
particle
swarm
optimisation
as
a
search
mechanism
</s>
<s>
The
experimental
results
on
15
different
datasets
show
that
KDE
can
work
well
on
both
continuous
and
discrete
datasets
</s>
<s>
In
addition
feature
subsets
evolved
by
KDE
achieves
similar
or
better
classification
performance
than
the
counting
approach
</s>
<s>
Furthermore
the
results
on
artificial
datasets
with
various
interactions
show
that
KDE
is
able
to
capture
correctly
the
interaction
between
features
in
both
relevance
and
redundancy
which
can
not
be
achieved
by
using
the
counting
approach
.
</s>