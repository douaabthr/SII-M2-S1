<s>
Evolution
of
visual
controllers
for
obstacle
avoidance
in
mobile
robotics
</s>
<s>
The
purpose
of
this
work
is
to
automatically
design
vision
algorithms
for
a
mobile
robot
adapted
to
its
current
visual
context
</s>
<s>
In
this
paper
we
address
the
particular
task
of
obstacle
avoidance
using
monocular
vision
</s>
<s>
Starting
from
a
set
of
primitives
composed
of
the
different
techniques
found
in
the
literature
we
propose
a
generic
structure
to
represent
the
algorithms
using
standard
resolution
video
sequences
as
an
input
and
velocity
commands
to
control
a
wheel
robot
as
an
output
</s>
<s>
Grammar
rules
are
then
used
to
construct
correct
instances
of
algorithms
that
are
then
evaluated
using
different
protocols
evaluation
of
trajectories
performed
in
a
goal
reaching
task
or
imitation
of
a
hand-guided
trajectory
</s>
<s>
A
genetic
program
is
applied
to
evolve
populations
of
algorithms
in
order
to
optimize
the
performances
of
the
controllers
</s>
<s>
The
first
results
obtained
in
a
simulated
environment
show
that
the
evolution
produces
algorithms
that
can
be
easily
interpreted
and
which
are
clearly
adapted
to
the
visual
context
</s>
<s>
However
the
resulting
trajectories
are
often
erratic
and
the
generalization
capacities
are
poor
</s>
<s>
To
improve
the
results
we
propose
to
use
a
two-phase
evolution
combining
imitation
and
goal
reaching
evaluations
and
to
add
some
constraints
in
the
grammar
rules
to
enforce
a
more
generic
behavior
</s>
<s>
The
results
obtained
in
simulation
show
that
the
evolved
algorithms
are
more
efficient
and
more
generic
</s>
<s>
Finally
we
apply
the
imitation
based
evolution
on
real
sequences
and
test
the
evolved
algorithms
on
a
real
robot
</s>
<s>
Though
simplified
by
dropping
the
goal
reaching
constraint
the
resulting
algorithms
behave
well
in
a
corridor
centering
task
and
show
certain
generalization
capacities
.
</s>