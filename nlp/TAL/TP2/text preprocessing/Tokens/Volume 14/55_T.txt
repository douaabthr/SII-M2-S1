<s>
Distributed
deduplication
with
fingerprint
index
management
model
for
big
data
storage
in
the
cloud
</s>
<s>
As
data
progressively
grows
within
data
centers
the
cloud
storage
models
face
several
issues
while
storing
data
and
offers
abilities
needed
to
shift
data
in
an
adequate
time
frame
</s>
<s>
This
study
aims
to
develop
a
distributed
deduplication
model
to
achieve
scalable
throughput
and
capacity
utilizing
many
data
servers
for
duplicating
data
in
parallel
with
minimal
loss
</s>
<s>
This
paper
proposes
a
new
cloud
storage
model
based
on
a
distributed
deduplication
with
the
fingerprint
index
management
DDFI
model
</s>
<s>
The
DDFI
model
operates
on
three
main
stages
</s>
<s>
At
the
initial
stage
the
DDFI
model
makes
use
of
an
effective
routing
technique
depending
upon
the
similarity
level
of
the
data
which
leads
to
low
network
overhead
by
rapid
identification
of
storage
locations
</s>
<s>
In
the
second
stage
the
duplicate
data
identification
procedure
is
carried
out
by
the
use
of
the
MD5
algorithm
</s>
<s>
At
the
final
stage
a
fingerprint
index
management
process
is
executed
where
a
fingerprint
index
comprises
fingerprints
and
its
corresponding
position
details
of
every
written
chunk
</s>
<s>
For
optimizing
the
results
of
the
deduplication
performance
the
DDFI
model
manages
the
fingerprint
index
in
storage
space
and
only
sometimes
writes
to
disk
at
the
same
time
as
the
cloud
database
scheme
is
idle
</s>
<s>
The
simulation
outcome
exhibited
that
the
presented
DDFI
model
offered
maximum
results
with
a
higher
deduplication
ratio
DR
with
a
minimum
overhead
of
network
bandwidth
</s>
<s>
From
the
detailed
comparative
analysis
it
is
inferred
that
the
presented
DFFI
model
offered
maximum
relative
DR
maximum
duplication
performance
minimum
read
bandwidth
and
write
bandwidth
.
</s>