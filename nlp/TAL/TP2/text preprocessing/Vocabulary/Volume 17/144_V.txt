.
1st
2nd
</s>
<s>
AdaBelief
Adam
Additionally
AngularGrad
By
CIFAR10
CIFAR100
CNN
Cos
DenseNet121
Due
For
ImageNet
In
MNIST
RAdam
ResNet16
ResNet18
ResNet50
Rosenbrock
SGD
SGDM
Source
Tan
The
To
VGG34
Yogi
a
according
accuracy
adaptive
additional
adjusting
also
an
and
angular
applying
approaches
areas
attain
based
been
best
better
both
but
calculation
calculations
can
changes
changing
classification
code
com/UtpalNandi/sqFm-A-novel-adaptive-optimization-scheme-for-deep-learning-model
comparing
comparison
consistently
contrast
convergence
dataset
datasets
deep
delivers
demonstrated
demonstrates
depending
development
diffGrad
does
during
effective
empirical
factors
first
for
function
functions
github
gives
global
good
gradient
gradually
has
https
image
important
in
is
it
learning
less
limitations
link
long
loss
maximizes
method
minima
minimizes
minimum
model
moments
momentum
most
necessitates
network
networks
non-convex
non-negative
not
novel
of
on
one
optimization
order
overcome
overshoot
parameter
performance
phases
present
proposed
require
required
research
result
scheme
size
smoother
sqFm
square
step
study
suggested
take
tan
technique
than
that
the
these
those
time
to
training
trajectories
trajectory
update
uses
using
value
values
when
which
with
zigzag