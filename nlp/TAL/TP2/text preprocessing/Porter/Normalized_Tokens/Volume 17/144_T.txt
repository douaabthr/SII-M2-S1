<s>
sqfm
a
novel
adapt
optim
scheme
for
deep
learn
model
</s>
<s>
for
deep
model
train
an
optim
techniqu
is
requir
that
minim
loss
and
maxim
accuraci
</s>
<s>
the
develop
of
an
effect
optim
method
is
one
of
the
most
import
studi
area
</s>
<s>
the
diffgrad
optim
method
use
gradient
chang
dure
optim
phase
but
doe
not
updat
2nd
order
moment
base
on
1st
order
moment
and
the
angulargrad
optim
method
use
the
angular
valu
of
the
gradient
which
necessit
addit
calcul
</s>
<s>
due
to
these
factor
both
of
those
approach
result
in
zigzag
trajectori
that
take
a
long
time
and
requir
addit
calcul
to
attain
a
global
minimum
</s>
<s>
to
overcom
those
limit
a
novel
adapt
deep
learn
optim
method
base
on
squar
of
first
momentum
sqfm
ha
been
propos
</s>
<s>
by
adjust
2nd
order
moment
depend
on
1st
order
moment
and
chang
step
size
accord
to
the
present
gradient
on
the
non-neg
function
the
suggest
sqfm
deliv
a
smoother
trajectori
and
better
imag
classif
accuraci
</s>
<s>
the
empir
research
compar
the
perform
of
the
propos
sqfm
with
adam
diffgrad
and
angulargrad
appli
non-convex
function
demonstr
that
the
suggest
method
deliv
the
best
converg
and
paramet
valu
</s>
<s>
in
comparison
to
sgd
adam
diffgrad
radam
and
angulargrad
tan
use
the
rosenbrock
function
the
propos
sqfm
method
can
attain
the
global
minima
gradual
with
less
overshoot
</s>
<s>
addit
it
is
demonstr
that
the
propos
sqfm
give
consist
good
classif
accuraci
when
train
cnn
network
resnet16
resnet50
vgg34
resnet18
and
densenet121
on
the
cifar10
cifar100
and
mnist
dataset
in
contrast
to
sgdm
diffgrad
adam
angulargrad
co
and
angulargrad
tan
</s>
<s>
the
propos
method
also
give
the
best
classif
accuraci
than
sgd
adam
adabelief
yogi
radam
and
angulargrad
use
the
imagenet
dataset
on
the
resnet18
network
</s>
<s>
sourc
code
link
http
github
.
com/utpalnandi/sqfm-a-novel-adaptive-optimization-scheme-for-deep-learning-model
.
</s>