<s>
orad
a
new
framework
of
offlin
reinforc
learn
with
q-valu
regular
</s>
<s>
offlin
reinforc
learn
rl
defin
a
framework
for
learn
from
previous
collect
static
buffer
</s>
<s>
howev
offlin
rl
is
prone
to
approxim
error
caus
by
out-of-distribut
ood
data
and
particular
ineffici
for
pixel-bas
learn
task
compar
with
state-bas
input
control
method
</s>
<s>
sever
pioneer
effort
have
been
made
to
solv
this
problem
some
use
pessimist
q-valu
approxim
for
unseen
observ
while
other
train
a
model
to
simul
the
environ
to
train
a
model
on
previous
collect
data
to
learn
polici
</s>
<s>
howev
these
method
requir
accur
and
time-consum
estim
of
the
q-valu
or
the
environ
model
</s>
<s>
base
on
this
observ
we
present
offlin
rl
method
with
augment
data
orad
a
handi
but
non-trivi
extens
to
offlin
rl
algorithm
</s>
<s>
we
show
that
simpl
data
augment
e.
g
</s>
<s>
random
translat
and
random
crop
signific
elev
the
perform
of
the
state-of-the-art
offlin
rl
algorithm
</s>
<s>
besid
we
find
that
regular
of
the
q-valu
can
also
enhanc
perform
</s>
<s>
extens
experi
on
the
pixel-bas
input
control-atari
demonstr
the
superior
of
orad
over
sota
offlin
rl
method
consid
both
perform
and
data
effici
and
reveal
that
orad
is
more
effect
for
the
pixel-bas
control
.
</s>