Controlling the performance of deep neural networks using a single parameter.
In this paper, we propose a fast and controllable performance adjustment scheme for the task of hierarchical model distribution. Instead of model retraining or pruning, our scheme flexibly adjusts the model performance into different levels by using a single parameter. In particular, we perturb the weights of the last fully connected layer of the model, which is driven according to a hyperparameter. We take advantage of the KL divergence for detailed analysis, where we analytically establish the relationship between the loss and the hyperparameter. Eventually, we run experiments on VGGNet, ResNet, Densenet, and SENet on three popular datasets, the results of which demonstrate the effectiveness of our method in controlling the performance of the neural network models.
